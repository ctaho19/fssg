Hi,

Please read and reference the below to understand the contents of the attached folders for context. As you ingest the code, please focus on clearly understanding the intent and architecture of the pipeline and how the files mesh.

Context:
Audience:
Engineers and the Automated Monitoring TeamBackground:An initiative is underway to establish Controls Health monitoring data across all Technology Controls. This data will be visualized into a ‘single pane of glass’ view. This will enable teams to monitor their control portfolio, complete with alerting and notifications to drive actions to maintain control effectiveness.A centrally managed Controls Health Monitoring Program will deliver this effort and consists of two workstreams 1) Manual / Hybrid Control Solutions and 2) Automated Solutions.This documentation is focused on the Automated solutions that will apply across all Automated Technology Controls.Purpose:
This document is intended to explain the process of developing a pipeline on ETIP for Automated Control Health Monitoring. The goal of this document is to provide engineers and the Automated Monitoring Team with the necessary information to onboard their locally developed ETL jobs on to our Github repository, develop the required tests, and open a PR to get their changes reviewed.Process Overview:
Pipeline development is a necessary part of onboarding backbook controls onto the Automated monitoring program, as described in the internal onboarding process document.
As part of the onboarding process, ETL jobs for the control must be developed that capture the appropriate monitoring tiersDevelopment should be done utilizing QA data, when available.Development is complete once you are able to calculate all monitoring tiers for the control.For Upon Occurrence and Continuous controls, you will need to develop Tier 1 and Tier 2 metrics. For all other controls you will need to develop Tier 0, Tier 1, and Tier 2 metrics. 
Example Pipeline: https://github.cloud.capitalone.com/ETIP/etip-data-pipelines/tree/main/src/pipelines/pl_automated_monitoring_cloud_custodianGuide:Follow the instructions provided by AxDP for setting up your local environmentPart of this process involves forking the existing ETIP data pipelines repository. You may either set up your own repo, or request to join the existing Automated Monitoring repository on GithubIf the latter option is preferred, reach out to Zachary Engle (CONT)
Once you have cloned the repository to your local machine, create a new branch to begin development
Create a folder for the pipeline under “src/pipelines”. The name of the pipeline will depend on its scope:If the pipeline will only generate monitoring metrics for one control, the folder should be named in the format of: pl_automated_monitoring_[CTRL_ID]Example: pl_automated_monitoring_ctrl_1074653If the pipeline sources monitoring data for multiple like controls from a shared location, the pipeline should be named to represent this grouping of controlsExample: pl_automated_monitoring_cloud_custodian
While you may develop your pipeline in a number of ways as described in the AxDP documentation, this guide follows the recommended Config Pipeline approachavro_schema.json - a json file containing the schema for the output datasetThat schema can be pulled from here config.yml - a yml file containing the extracts, transforms, and loads that will need to happen for your pipeline. Please use config.yml as an exampleFor more information reference the ETIP documentation Utilize the same name for your pipeline (line 2 in the yml file)  as you do for the folder that your pipeline lives in“Etip_controls_monitoring_metrics” should be the destination dataset used for the load steppipeline.py - a Python file that triggers your pipeline runPlease reference this pipeline filesql – a sub-folder containing the various SQL files needed for the pipeline. For Snowflake and Postgres data sources ONLYtransform.py - a python file containing the transformation logic that is needed to produce your metricsWhile transformations can be written in pipeline.py, it is often easier to break them out into their own filePlease reference this transform file
After you have created each of these files, you will have to set up unit tests in order to reach the 80% pipeline code coverage that ETIP and Capital One requiresCreate a file under test/pipelines and name it test_[NAME_OF_PIPELINE].pyExample: test_pl_automated_monitoring_cloud_custodian.py For reference please utilize the test_pl_automated_monitoring_cloud_custodian.py fileYour unit tests should ensure that the transformation logic is implemented properly, and that the output received matches the expected outputAt least one unit test must invoke the “run” command from pipeline.py
Run your tests via the following command:pipenv run pytest ./tests/pipelines/[your_test_file].pyOnce you successfully run your test file, create a Pull Request from your branch to the develop branch of the ETIP/etip-pipelines repositoryPost a link to your PR in the #axdp-innersourcing channelIn your message to #axdp-innersourcing please CC Zachary Engle (CONT) so that the TCAM team may track the progress of your PR