Now acknowledge these three files for the original ctrl-1077188 and my new revised version that I am working on, and compile them into a markdown file that compares the two and gives steps for creating the new code for all three tiers integrating the api call instead of sql only:

I should emphasize that the final code for the metrics for this control will use the dataset directly and not a .csv file, the csv file is a temporary solution as I was building out the logic.

Tier 0 can be the same as it validates that the control is operating, so ensuring that certificates are being rotated is fine

Tier 1 will ensure that every in-scope cert from the API is reflected in the dataset. Thus far I have had trouble validating what an "in-scope" cert is as there are more certs returned from the API call than distinct certificates in the dataset. I was attempting to find the common configurations for what all the certificates in the dataset had to identify the filter criteria to ensure that all in-scope certificatess are accounted for.

Tier 2 will ensure that every in-scope certificate that was found in the dataset is being properly rotated within 13 months and that none of them are being used past their expiration date.

Original:

CTRL_1077188="""
WITH EXPIRED_CERTS AS (
    SELECT 
        c1.Certificate_ID,
        c1.Certificate_ARN,
        c1.NOT_VALID_BEFORE_UTC_TIMESTAMP,
        c1.NOT_VALID_AFTER_UTC_TIMESTAMP as EXPIRY_DATE,
        c1.LAST_USAGE_OBSERVATION_UTC_TIMESTAMP,
        -- Check for rotation
        LEAD(c1.NOT_VALID_BEFORE_UTC_TIMESTAMP) OVER (PARTITION BY c1.Certificate_ARN ORDER BY c1.NOT_VALID_BEFORE_UTC_TIMESTAMP) as ROTATION_DATE
    FROM CYBR_DB.PHDP_CYBR.CERTIFICATE_CATALOG_CERTIFICATE_USAGE c1
    WHERE c1.CERTIFICATE_ARN LIKE '%arn:aws:acm%'
    AND c1.NOT_VALID_AFTER_UTC_TIMESTAMP < CURRENT_TIMESTAMP
    AND c1.NOT_VALID_AFTER_UTC_TIMESTAMP >= DATEADD(day, -3, CURRENT_TIMESTAMP)
    AND c1.LAST_USAGE_OBSERVATION_UTC_TIMESTAMP > c1.NOT_VALID_AFTER_UTC_TIMESTAMP
    AND DATEDIFF(day, c1.NOT_VALID_BEFORE_UTC_TIMESTAMP, c1.NOT_VALID_AFTER_UTC_TIMESTAMP) BETWEEN 350 AND 380
),
TIER_0 AS (
    SELECT
        CURRENT_TIMESTAMP() as DATE,
        'CTRL-1077188' AS CTRL_ID,
        'MNTR-1077188-T0' as MONITORING_METRIC_NUMBER,
        -- Metric fails if we find expired certs that were used after expiry without rotation
        CASE 
            WHEN COUNT(CASE WHEN ROTATION_DATE IS NULL OR ROTATION_DATE > EXPIRY_DATE THEN 1 END) > 0 THEN 0
            ELSE 100
        END as MONITORING_METRIC,
        CASE 
            WHEN COUNT(CASE WHEN ROTATION_DATE IS NULL OR ROTATION_DATE > EXPIRY_DATE THEN 1 END) > 0 THEN 'RED'
            ELSE 'GREEN'
        END as COMPLIANCE_STATUS,
        CASE 
            WHEN COUNT(CASE WHEN ROTATION_DATE IS NULL OR ROTATION_DATE > EXPIRY_DATE THEN 1 END) > 0 THEN 0
            ELSE 1
        END as NUMERATOR,
        1 as DENOMINATOR
    FROM EXPIRED_CERTS
),

-- Tier 1 Query:
CERT_STATUS_T1 AS (
    SELECT
        SUM(CASE WHEN NOT_VALID_AFTER_UTC_TIMESTAMP IS NULL THEN 1 ELSE 0 END) AS NULL_CERTS,
        COUNT(DISTINCT CERTIFICATE_ID) AS TOTAL_CERTS
    FROM CYBR_DB.PHDP_CYBR.CERTIFICATE_CATALOG_CERTIFICATE_USAGE
    WHERE CERTIFICATE_ARN LIKE '%arn:aws:acm%'
    AND NOT_VALID_AFTER_UTC_TIMESTAMP >= DATEADD('DAY', -365, CURRENT_TIMESTAMP)
),
TIER_1 AS (
    SELECT
        CURRENT_TIMESTAMP AS DATE,
        'CTRL-1077188' AS CTRL_ID,
        'MNTR-1077188-T1' AS MONITORING_METRIC_NUMBER,
        CASE 
            WHEN TOTAL_CERTS > 0 THEN ROUND(((TOTAL_CERTS - NULL_CERTS) * 100.0 / TOTAL_CERTS), 2)
            ELSE 0 
        END AS MONITORING_METRIC,
        CASE
            WHEN NULL_CERTS > 0 THEN 'RED'
            ELSE 'GREEN'
        END AS COMPLIANCE_STATUS,
        TOTAL_CERTS - NULL_CERTS AS NUMERATOR,
        TOTAL_CERTS AS DENOMINATOR
    FROM CERT_STATUS_T1
),

-- Tier 2 Query:
CERT_STATUS_T2 AS (
    SELECT
        COUNT(DISTINCT CASE WHEN NOT_VALID_AFTER_UTC_TIMESTAMP >= LAST_USAGE_OBSERVATION_UTC_TIMESTAMP THEN CERTIFICATE_ID ELSE NULL END) AS COMPLIANT_CERTS,
        COUNT(DISTINCT CASE WHEN NOT_VALID_AFTER_UTC_TIMESTAMP >= DATEADD('DAY', -365, CURRENT_TIMESTAMP) THEN CERTIFICATE_ID ELSE NULL END) AS TOTAL_CERTS
    FROM CYBR_DB.PHDP_CYBR.CERTIFICATE_CATALOG_CERTIFICATE_USAGE
    WHERE CERTIFICATE_ARN LIKE '%arn:aws:acm%'
    AND NOT_VALID_AFTER_UTC_TIMESTAMP >= DATEADD('DAY', -365, CURRENT_TIMESTAMP)
),
TIER_2 AS (
    SELECT
        CURRENT_TIMESTAMP AS DATE,
        'CTRL-1077188' AS CTRL_ID,
        'MNTR-1077188-T2' AS MONITORING_METRIC_NUMBER,
        ROUND((COMPLIANT_CERTS * 100.0 / TOTAL_CERTS), 2) AS MONITORING_METRIC,
        CASE WHEN (TOTAL_CERTS - COMPLIANT_CERTS) > 0 THEN 'RED'
        ELSE 'GREEN' END AS COMPLIANCE_STATUS,
        TO_DOUBLE(COMPLIANT_CERTS) AS NUMERATOR,
        TOTAL_CERTS AS DENOMINATOR
    FROM CERT_STATUS_T2
)

-- Final Combined Output
SELECT * FROM TIER_0
UNION ALL
SELECT * FROM TIER_1
UNION ALL
SELECT * FROM TIER_2;
"""

New (In Progress):

import os
import time
import json
import pandas as pd
import requests

# File paths
DATASET_CSV_FILE = "/Users/uno201/Documents/Controls Monitoring Automation/CTRL-1077188/dataset_arns.csv"
OUTPUT_MATCHING_METADATA_CSV = "/Users/uno201/Documents/Controls Monitoring Automation/CTRL-1077188/matching_certs_metadata.csv"
OUTPUT_NON_MATCHING_METADATA_CSV = "/Users/uno201/Documents/Controls Monitoring Automation/CTRL-1077188/non_matching_certs_metadata.csv"
OUTPUT_DIFFERENCES_CSV = "/Users/uno201/Documents/Controls Monitoring Automation/CTRL-1077188/configuration_differences.csv"

# API details
API_URL = "https://api.cloud.capitalone.com/internal-operations/cloud-service/aws-tooling/search-resource-configurations"
API_HEADERS = {
    'Accept': 'application/json;v=1.0',
    'Authorization': 'Bearer ',
    'Content-Type': 'application/json'
}
API_DETAIL_TIMEOUT = 60  # Timeout for individual metadata fetches
API_CALL_DELAY = 0.15  # Delay between API calls to avoid rate limits

# Load dataset ARNs
def load_dataset_arns(filepath):
    dataset_df = pd.read_csv(filepath)
    return set(dataset_df["CERTIFICATE_ARN"])

# Fetch all certificates from the API with pagination
def fetch_all_certificates():
    all_certificates = []
    next_record_key = None

    while True:
        url = API_URL
        if next_record_key:
            url += f"?limit=10000&nextRecordKey={next_record_key}"

        payload = {
            "searchParameters": [
                {
                    "resourceType": "AWS::ACM::Certificate",
                    "configurationItems": [
                        {
                            "configurationName": "issuer",
                            "configurationValue": "Amazon"
                        }
                    ],
                    "assignmentGroupDisplayId": "SECURITY_ENCRYPTION_SERVICES",
                    "businessApplicationName": "BAACMAUTOMATION",
                    "lineOfBusiness": "Cyber",
                    "technologyDivision": "Cyber Tech",
                    "paymentCardIndustryCategorization": "2",
                    "multiAvalabilityZoneDeployment": "False"
                }
            ]
        }

        try:
            response = requests.post(url, headers=API_HEADERS, data=json.dumps(payload), timeout=API_DETAIL_TIMEOUT, verify=False)
            response.raise_for_status()
            data = response.json()

            if "resourceConfigurations" in data:
                all_certificates.extend(data["resourceConfigurations"])

            next_record_key = data.get("nextRecordKey")
            if not next_record_key:
                break

        except requests.exceptions.RequestException as e:
            print(f"API call failed: {e}")
            break

        time.sleep(API_CALL_DELAY)

    return pd.DataFrame(all_certificates)

# Fetch metadata for a list of ARNs
def fetch_metadata_for_arns(arns):
    metadata = []
    for idx, arn in enumerate(arns, start=1):
        print(f"Fetching metadata for ARN {idx}/{len(arns)}: {arn}")
        payload = {
            "searchParameters": [
                {
                    "amazonResourceName": arn
                }
            ]
        }
        try:
            response = requests.post(API_URL, headers=API_HEADERS, data=json.dumps(payload), timeout=API_DETAIL_TIMEOUT, verify=False)
            response.raise_for_status()
            data = response.json()
            if "resourceConfigurations" in data:
                metadata.extend(data["resourceConfigurations"])
        except requests.exceptions.RequestException as e:
            print(f"Failed to fetch metadata for ARN {arn}: {e}")
        time.sleep(API_CALL_DELAY)
    return pd.DataFrame(metadata)

# Compare configurations between two groups
def compare_configurations(matching_metadata, non_matching_metadata, output_file):
    matching_df = pd.read_csv(matching_metadata)
    non_matching_df = pd.read_csv(non_matching_metadata)

    differences = {}
    for column in matching_df.columns:
        matching_values = set(matching_df[column].dropna().unique())
        non_matching_values = set(non_matching_df[column].dropna().unique())
        if matching_values != non_matching_values:
            differences[column] = {
                "matching_values": list(matching_values),
                "non_matching_values": list(non_matching_values)
            }

    # Save differences to a CSV file
    differences_df = pd.DataFrame([
        {"Configuration": key, "Matching Values": value["matching_values"], "Non-Matching Values": value["non_matching_values"]}
        for key, value in differences.items()
    ])
    differences_df.to_csv(output_file, index=False)
    print(f"Differences saved to {output_file}")

# Main execution
def main():
    # Load dataset ARNs
    dataset_arns = load_dataset_arns(DATASET_CSV_FILE)
    print(f"Loaded {len(dataset_arns)} ARNs from the dataset.")

    # Fetch all certificates from the API
    all_certificates = fetch_all_certificates()
    print(f"Fetched {len(all_certificates)} certificates from the API.")

    # Separate matching and non-matching certificates
    matching_arns = dataset_arns.intersection(set(all_certificates["amazonResourceName"]))
    non_matching_arns = set(all_certificates["amazonResourceName"]) - matching_arns

    print(f"Found {len(matching_arns)} matching certificates.")
    print(f"Found {len(non_matching_arns)} non-matching certificates.")

    # Fetch metadata for matching and non-matching certificates
    matching_metadata = fetch_metadata_for_arns(matching_arns)
    non_matching_metadata = fetch_metadata_for_arns(non_matching_arns)

    # Save metadata to CSV
    if not matching_metadata.empty:
        matching_metadata.to_csv(OUTPUT_MATCHING_METADATA_CSV, index=False)
        print(f"Matching metadata saved to {OUTPUT_MATCHING_METADATA_CSV}")
    if not non_matching_metadata.empty:
        non_matching_metadata.to_csv(OUTPUT_NON_MATCHING_METADATA_CSV, index=False)
        print(f"Non-matching metadata saved to {OUTPUT_NON_MATCHING_METADATA_CSV}")

    # Compare configurations
    if not matching_metadata.empty and not non_matching_metadata.empty:
        compare_configurations(OUTPUT_MATCHING_METADATA_CSV, OUTPUT_NON_MATCHING_METADATA_CSV, OUTPUT_DIFFERENCES_CSV)

if __name__ == "__main__":
    main()

import requests
import json
import os
import datetime
import csv
from dotenv import load_dotenv
from urllib.parse import urlencode # To help build query strings safely


# CloudRadar API Details from Environment Variables
API_TOKEN = "eyJhbGciOiJkaXIiLCJ0diI6Miwia2lkIjoicjRxIiwiZW5jIjoiQTEyOENCQy1IUzI1NiIsInBjayI6MX0..dy56eqPqLXA_mEBcMkp7XQ.5Tv2ZR_FAdQY46P3Tn3TxPTrG0QRpD2_1CQt5u7aL_kUJU9PcZgH7taiZ3OzxxRbcdFYdHtVvsz5DGh2KahiZ_OtNiGmD9XK2GJpK9p3wOiMtEiTckzk_hu86-LF0TUxu2Y1zoo0En4s29SNNkxrhVJic9jp8oB6IHTYDGtzAyWXU0XLyaKa4aYP_M6CYco7j3qrisJvDsVsV4x71Km0OFQvtvRw6ezICG7kMpLmfrS462RLEYUOCdOv2HawnZrLWHdW7-aJysW6TOLaRPMZcu2Xc_azbJNQLwQ2QczvMDRDYvuWWZJLwGiPj9yM3YK1.HNhkPkjCD3ewhKjXGF2etA"
# Base URL *without* any query parameters
API_BASE_URL = "https://api.cloud.capitalone.com/internal-operations/cloud-service/aws-tooling/search-resource-configurations"
API_HEADERS = {
    'Accept': 'application/json;v=1.0',
    'Authorization': f'Bearer {API_TOKEN}',
    'Content-Type': 'application/json'
}
API_LIMIT = 10000
DATASET_CSV_FILE = "/Users/uno201/Documents/Controls Monitoring Automation/CTRL-1077188/dataset_arns.csv"  # <--- REPLACE THIS ENTIRE STRING
CSV_ARN_COLUMN_NAME = "CERTIFICATE_ARN" # Column header in your CSV
OUTPUT_FILE = "tier1_test_results.txt" # Optional: File to write results to

print(f"Using explicit CSV file path: {DATASET_CSV_FILE}")
if not os.path.exists(DATASET_CSV_FILE):
     print(f"ERROR: The specified CSV file path does not exist: {DATASET_CSV_FILE}")
     print("Please double-check the path.")
     # Optional: exit if the file doesn't exist before trying to read it
     # exit(1) # Uncomment this line to stop the script if the path is wrong

# Check if essential variables are loaded
if not API_TOKEN:
    print("ERROR: Missing CLOUDRADAR_API_TOKEN in .env file or environment variables.")
    exit(1)

# --- CORRECTED Helper Function to Fetch CloudRadar ARNs ---
def fetch_cloudradar_arns(base_url, headers, limit):
    """
    Fetches all AWS::ACM::Certificate ARNs issued by Amazon from CloudRadar,
    handling pagination via URL query parameters.
    """
    all_arns = set()
    next_key = None
    page_count = 0
    current_url = base_url # Start with the base URL for the first request

    # Corrected Payload structure
    payload = {
        "responseFields": ["amazonResourceName"],
        "searchParameters": [{
            "resourceType": "AWS::ACM::Certificate",
            "configurationItems": [{  # Corrected key
                "configurationName": "issuer", # Corrected key
                "configurationValue": "Amazon"
            }]
        }],
        "limit": limit # Keep limit in the payload as per original example
    }
    payload_json = json.dumps(payload) # Prepare payload once

    while True:
        page_count += 1
        print(f"--- Fetching Page {page_count} ---")
        print(f"URL: {current_url}")
        # print(f"Payload: {payload_json}") # Optional: uncomment to debug payload

        try:
            # Make the POST request to the current URL (which includes nextRecordKey after 1st page)
            response = requests.post(current_url, headers=headers, data=payload_json, timeout=180, verify=False)
            response.raise_for_status() # Check for HTTP errors
            data = response.json()

            resources = data.get("resourceConfigurations", [])
            fetched_count = 0
            if resources:
                for config in resources:
                    # Add extra checks for robustness
                    if isinstance(config, dict) and "amazonResourceName" in config and config["amazonResourceName"]:
                        all_arns.add(config["amazonResourceName"])
                        fetched_count += 1
                    # else: # Optional: Log unexpected item format
                    #     print(f"Warning: Skipping unexpected item format in page {page_count}: {config}")


            # Extract the nextRecordKey *from the response*
            next_key = data.get("nextRecordKey")

            print(f"Page {page_count} fetched. Found {fetched_count} valid items. Next key exists: {bool(next_key)}")

            # --- Pagination Logic ---
            if next_key:
                # If there's a next key, prepare the URL for the *next* iteration
                # by adding it as a query parameter to the *base* URL.
                query_params = urlencode({'nextRecordKey': next_key})
                current_url = f"{base_url}?{query_params}"
            else:
                # If no next key in the response, we are done.
                print("No nextRecordKey found in response. Pagination complete.")
                break # Exit the loop

        except requests.exceptions.Timeout:
            print(f"ERROR: CloudRadar API request timed out on page {page_count}.")
            print("Returning ARNs fetched so far (might be incomplete).")
            return all_arns, False # Indicate failure

        except requests.exceptions.RequestException as e:
            print(f"ERROR: CloudRadar API request failed: {e}")
            if hasattr(e, 'response') and e.response is not None:
                print(f"Response Status Code: {e.response.status_code}")
                # Try to decode response as JSON for error details if possible
                try:
                    error_details = e.response.json()
                    print(f"Response JSON: {json.dumps(error_details, indent=2)}")
                except json.JSONDecodeError:
                    print(f"Response Text: {e.response.text[:1000]}...") # Log more text on error
            print("Returning ARNs fetched so far (might be incomplete).")
            return all_arns, False # Indicate failure

        except json.JSONDecodeError as e:
            print(f"ERROR: Failed to decode JSON response from CloudRadar API on page {page_count}: {e}")
            # Ensure 'response' exists before trying to access its text attribute
            if 'response' in locals() and hasattr(response, 'text'):
                 print(f"Response Text: {response.text[:500]}...")
            return all_arns, False # Indicate failure
        # --- End of Loop ---

    print(f"\nFinished fetching from CloudRadar. Total distinct ARNs found: {len(all_arns)}")
    return all_arns, True # Return True for success

# --- Helper Function to Load Dataset ARNs from CSV (Same as before) ---
def load_dataset_arns_from_csv(filepath, arn_column_name):
    """Loads ARNs from a CSV file, expecting a specific column header."""
    dataset_arns = set()
    print(f"\nLoading dataset ARNs from CSV file: {filepath}")
    try:
        if not os.path.exists(filepath):
            print(f"ERROR: Dataset CSV file not found at '{filepath}'")
            return dataset_arns, False

        with open(filepath, mode='r', newline='', encoding='utf-8-sig') as csvfile: # Use utf-8-sig to handle potential BOM
            reader = csv.reader(csvfile)
            try:
                header = next(reader) # Read the header row
            except StopIteration:
                print(f"ERROR: CSV file '{filepath}' appears to be empty.")
                return dataset_arns, False # Empty file

            # Find the index of the ARN column (case-insensitive strip)
            header_processed = [h.strip() for h in header]
            arn_column_name_stripped = arn_column_name.strip()
            try:
                arn_column_index = header_processed.index(arn_column_name_stripped)
                print(f"Found ARN column '{arn_column_name_stripped}' at index {arn_column_index}.")
            except ValueError:
                print(f"ERROR: Column '{arn_column_name_stripped}' not found in CSV header: {header_processed}")
                print(f"Please ensure the CSV file '{filepath}' has the correct header.")
                return dataset_arns, False

            # Read the rest of the rows
            row_count = 0
            valid_arn_count = 0
            for row in reader:
                row_count += 1
                if len(row) > arn_column_index:
                    arn = row[arn_column_index].strip() # Strip whitespace from ARN
                    if arn and arn.startswith("arn:aws:acm"): # Basic validation
                        dataset_arns.add(arn)
                        valid_arn_count += 1
                # else: # Optional: warn about short rows
                #     print(f"Warning: Row {row_count + 1} has fewer columns than expected: {row}")

        print(f"Processed {row_count} data rows from CSV.")
        print(f"Loaded {len(dataset_arns)} distinct and valid ACM ARNs from CSV file.")
        return dataset_arns, True
    except Exception as e:
        print(f"ERROR reading dataset CSV file '{filepath}': {e}")
        return dataset_arns, False

# --- Main Execution (No changes needed from here down) ---

.# 1. Fetch from CloudRadar (using the corrected function)
print("--- Step 1: Fetching Data from CloudRadar ---")
cloudradar_arns_set, radar_success = fetch_cloudradar_arns(API_BASE_URL, API_HEADERS, API_LIMIT)
if not radar_success:
    print("\nAborting due to failure fetching CloudRadar data.")
    exit(1)
cloudradar_count = len(cloudradar_arns_set)
print(f"CloudRadar distinct ARN count (Denominator): {cloudradar_count}")

# 2. Load Dataset ARNs from CSV
print("\n--- Step 2: Loading Dataset Data from CSV ---")
dataset_arns_set, load_success = load_dataset_arns_from_csv(DATASET_CSV_FILE, CSV_ARN_COLUMN_NAME)
if not load_success:
    print("\nAborting due to failure loading dataset CSV data.")
    exit(1)
# dataset_count = len(dataset_arns_set) # Not directly used in metric

# 3. Calculate Tier 1 Metric
print("\n--- Step 3: Calculating Tier 1 Metric ---")
if cloudradar_count > 0:
    matched_arns_set = cloudradar_arns_set.intersection(dataset_arns_set)
    numerator_t1 = len(matched_arns_set)
    denominator_t1 = float(cloudradar_count)
    metric_t1 = round((numerator_t1 * 100.0 / denominator_t1), 2) if denominator_t1 > 0 else 0.0
    status_t1 = "GREEN" if numerator_t1 == int(denominator_t1) else "RED"
else:
    print("CloudRadar reported 0 in-scope certificates. Setting Tier 1 to 100% GREEN.")
    numerator_t1 = 0
    denominator_t1 = 0.0
    metric_t1 = 100.0
    status_t1 = "GREEN"

# Format Tier 1 Output
current_date_str = datetime.date.today().isoformat()
tier1_result = {
    "DATE": current_date_str,
    "CTRL_ID": "CTRL-1077188",
    "MONITORING_METRIC_NUMBER": "MNTR-1077188-T1",
    "MONITORING_METRIC": float(metric_t1),
    "COMPLIANCE_STATUS": status_t1,
    "NUMERATOR": float(numerator_t1),
    "DENOMINATOR": float(denominator_t1)
}

# --- Output Results to Terminal ---
print("\n--- Tier 1 Metric Result (Compared to Dataset CSV) ---")
terminal_output = []
terminal_output.append("--- Tier 1 Metric Result ---")
for key, value in tier1_result.items():
    line = f"  {key}: {value}"
    print(line)
    terminal_output.append(line)

terminal_output.append("\nCalculation Summary:")
summary_lines = [
    f"  CloudRadar ARNs (Denominator): {int(denominator_t1)}",
    f"  Matched ARNs in Dataset CSV (Numerator): {int(numerator_t1)}",
    f"  Metric Value: {metric_t1}%",
    f"  Compliance Status: {status_t1}"
]
for line in summary_lines:
    print(line)
    terminal_output.append(line)

# 4. Generate Supporting Evidence (Missing ARNs)
print("\n--- Step 4: Generating Supporting Evidence (ARNs Missing from Dataset CSV) ---")
terminal_output.append("\n--- Supporting Evidence (Missing ARNs) ---")

missing_arns_set = cloudradar_arns_set.difference(dataset_arns_set)
missing_count = len(missing_arns_set)

if missing_count > 0:
    missing_header = f"\nFound {missing_count} ARN(s) in CloudRadar that are MISSING from the dataset CSV ('{DATASET_CSV_FILE}'):"
    print(missing_header)
    terminal_output.append(missing_header)
    count = 0
    for arn in sorted(list(missing_arns_set)):
        line = f"  - {arn}"
        print(line)
        terminal_output.append(line)
        count += 1
        if count >= 200:
            limit_msg = f"  ... (showing first 200 of {missing_count})"
            print(limit_msg)
            terminal_output.append(limit_msg)
            break
else:
    no_missing_msg = f"\nNo missing ARNs found. All CloudRadar ARNs are present in the dataset CSV ('{DATASET_CSV_FILE}')."
    print(no_missing_msg)
    terminal_output.append(no_missing_msg)

# --- Optionally Write to Output File ---
try:
    with open(OUTPUT_FILE, 'w') as f:
        f.write(f"Report generated on: {datetime.datetime.now().isoformat()}\n")
        f.write(f"Dataset CSV file used: {DATASET_CSV_FILE}\n")
        f.write("="*40 + "\n")
        f.write("\n".join(terminal_output))
        f.write("\n" + "="*40 + "\n")
    print(f"\nResults also written to file: {OUTPUT_FILE}")
except Exception as e:
    print(f"\nWarning: Could not write results to file '{OUTPUT_FILE}': {e}")

print("\n--- Script Execution Finished ---")

This is the query used to find dataset arns:
Select * from cybr_db.phdp_cybr.certificate_catalog_certificate_usage where certificate_arn like ‘%arn:aws:acm%’ and nominal_issuer = ‘Amazon’ AND not_valid_after_utc_timestamp >= DATEADD(‘DAY’, -365, CURRENT_TIMESTAMP)